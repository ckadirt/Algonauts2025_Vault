---
title: "Meeting: {{Meeting Title}}"
date: 2025-01-31
type: meeting
tags:
  - meeting
dg-publish: true
---

## Cesar  
**Potential way to organize using an Obsidian Vault:**  
- [GitHub Repository](https://github.com/ckadirt/Algonauts2025_Vault)  
- [Project Site](https://mellifluous-flan-118ac6.netlify.app/)  

---

## Mihir  
[GitHub Repository](https://github.com/PaulScotti/algonauts2025/tree/mihir)  

### fMRI Tokenization via VQ-VAE  
- **Tokenized fMRI → integrate with LLMs**  
- **N_tokens =** 32  
- **Variance:** 0.65 (Goal: 0.85)  

### MLP-Based Simple Model  
- Concatenates video, audio, and language features  
- Learnable language token  
- Heavy overfitting (consistent with MLP-based systems)  
- **Pearson_corr:** 0.211 (Subject 1, Season 7)  

### Data Pipeline  
- [GitHub Repository](https://github.com/PaulScotti/algonauts2025/tree/main)  
### Probably Better Way to Evaluate OOD  
- Train just on **Friends 1-6**, evaluate on **Movie 10**  
### Features  
- Different encoders: **Whisper, V-JEPA, Dino V2, Flan T5**  
### Architecture  
- **RNNs maybe?**  
### Window Stimuli  
- Original is **5**  
- Try increasing/decreasing and analyze effects of future features  

### More Data  
- Prepare preprocessing pipeline code (**voxels to parcels**)  
- [CNeuromod Video Games Data](https://www.cneuromod.ca/)  
- Train a **shared subject model**  

---

## Connor  

### fMRI Tokenization (Low Priority, More Risky)  

### Long Context Features with Transformers (Medium Priority)  
- **Once we beat baseline with better features, explore transformers.**  
- Long context was important in **Algonauts 2023**.  
- **Leverage long context:**  
  - Train a transformer on a sequence of `(num_modalities * num_frames + 1, dim)`, where the last token is a **CLS** for predicting **1000 parcel activity**.  
  - Different networks might attend to different features (e.g., visual network → vision features).  
  - The **1000 parcels** can be partitioned into **functional network clusters**.  
  - **Yeo 7 or 17 networks** already define these clusters.  
  - Append **n CLS tokens** for predicting each of the **n networks independently**, then combine predictions.  

### Modality-Specific Embeddings  
- Need **modality-specific linear projection** from modality feature dim to shared embedding dim.  
- Baseline uses **PCA**, but learning in the model end-to-end might be better.  
- **Attention Mechanism:**  
  - Can efficiently find the relevant signal in the feature history for prediction.  
  - Look at **what Huze did** for incorporating long context features:  
    - [Huze's Memory Encoding Model](https://github.com/huzeyann/MemoryEncodingModel)  
